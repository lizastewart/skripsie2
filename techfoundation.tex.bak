\chapter{Technical Foundation} \label{tech}
%(RIJK: net 'n comment om te se ek is baie lief vir jou en baie trots op jou <3)
In this chapter, detailed explanations of the algorithms and concepts applied throughout the project are presented. 
This chapter aims to enable the reader to better understand the implementation and reasoning for the use of specific methods.
\section{Thermal properties of timber}
Specific thermal properties of timber need to be thoroughly understood to allow for accurate modelling and interpretation of the results.
After $100 ^{^\circ}$C, the temperature exposure causes the timber structure to break down in such a way that it will not return to the original state after cooling.
This breakdown is generally referred to as thermal degradation which can be broken into four categories \citep{White:2001} \citep{Shi:2021}.
These four categories are based on the different reactions that occur, and are briefly summarised below:

\begin{itemize}
\item Above 100 \textdegree C and below 200 \textdegree C, the moisture in the timber is evaporated and other non-combustible gasses are released.
\item Between 200 \textdegree C and 300 \textdegree C, carbon monoxide is released in significant quantities, and some of the timber components undergo a change in chemical composition due to the high temperatures.
\item Temperatures above  300 \textdegree C and before 450\textdegree C are significant due to the amount of flammable volatiles released and the break of carbon linkages at 370 \textdegree C.
\item At temperatures higher than 450 \textdegree C, the remaining timber is charred and any further degradation is due to oxidation from carbon monoxide, carbon dioxide, and water.
\end{itemize}
%TODO
%These phases should be clear in the results and data. (TODO add lines for phases)
As the timber clearly undergoes physical and chemical changes as temperature changes it makes sense that the thermal properties also change with temperature.
Then logically the specific heat capacity($c_p$) and density($\rho$) of timber also varies with temperature.
As the thermal conductivity and thermal diffusivity will only be assessed at specific temperatures, the specific heat and density  at only those temperatures will be summarised below.
\begin{table}[H]\label{cptab}
\centering
	\begin{tabular}{ r r r r }
	\toprule
	\multicolumn{1}{c}{Temp}& \multicolumn{1}{c}{$c_p$}& \multicolumn{1}{c}{$\rho$} & \multicolumn{1}{c}{$c_p\rho$}\\
	\multicolumn{1}{c}{\textdegree C} & \multicolumn{1}{c}{$(J/kg/K)$} &  \multicolumn{1}{c}{$(kg/m^3)$} & \multicolumn{1}{c}{$(kJ/kg/K)$}\\
	\midrule
	0   & 1530 & 479&732.8\\
	60  &1652	& 479&791.3\\
	100 &13600& 479&6514.4\\
 	140 &2090 &408&852.7\\
	200 & 2000&408&816.0\\
	350 & 850 &220&187.0\\
	500 & 1200 &142&170.4\\
	800 & 1650&110&181.5\\
	1200& 1650 &100&165.0\\
	\bottomrule
	\end{tabular}
	\caption{$c_p$ and $\rho$ at different temperatures}
\end{table}
All the thermal properties of various timber species at high temperatures were discussed in a research article by \citet{Shi:2021}.
 In their article, they provide a general expression for the thermal conductivity of softwoods (Equation \ref{softconeq}) and char (Equation \ref{charconeq}).  


 \begin{equation}\label{softconeq}
 \kappa = 0.124 +0.8432 \times 10^{-4} (\text{T}-293) 
 \end{equation}
 \begin{equation}\label{charconeq}
 \kappa = 0.091 + 8.2 \times 10^{-4} \text{T}
 \end{equation}
 
 For this project, the temperature of char will be used as 300 \textdegree C following the research of \citeauthor{Westhuyzen:2020}. 
% The specific heat ($C_p$) will be assumed to be constant at 1500 kJ/$\text{m}^3$/K. the density of pine was also assumed constant at 
 Using the equations of \citeauthor{Shi:2021}, the following $\kappa$-values (Equation \ref{shiK}) are obtained. 
\begin{equation}\label{shiK}
  \kappa=
  \begin{blockarray}{*{1}{c} l}
%    \begin{block}{*{1}{>{$\footnotesize}c<{$}} 1}
%      $\text{W/m}\cdot\text{K}$ &\\
%    \end{block}
    \begin{block}{[*{1}{c}]>{$\footnotesize}l<{$}}
     	0.0993\bigstrut[t] & 0 $^{^\circ} \text{C}$\\
		0.1044& 60  $^{^\circ} \text{C}$\\ 
		0.1077& 100 $^{^\circ} \text{C}$\\ 
		0.1111& 140 $^{^\circ} \text{C}$\\ 
		0.1162& 200 $^{^\circ} \text{C}$\\ 
		0.3780& 350 $^{^\circ} \text{C}$\\
		0.5010& 500 $^{^\circ} \text{C}$\\ 
		0.7470& 800 $^{^\circ} \text{C}$\\ 
		1.0750& 1200 $^{^\circ} \text{C}$\\
    \end{block}
  \end{blockarray}
\end{equation}
From the big difference between these values and the Euro code values, it is clear that these simple equations are not sufficient for determining the thermal conductivity of pine.
\subsection{Heat conduction and diffusion}\label{heatconsec}	 
	Heat is transferred within an element if a temperature gradient present, the rate that heat transfers is directly influenced by the thermal conductivity of the material. 
	For this project heat transferred via conduction will be focused on but heat can also be transferred in the form of thermal radiation or convection \cite{Fish:2007}.
	The heat conduction equation or Fourier's Law of heat conduction \citep{Fourier:1878} can be simply expressed in  equation \ref{fourier_eq}. 
	Where heat flux is expressed as a partial differential equation dependant on the temperature and thickness of the element. 
	More information can be added if the temperature only varies in one direction. 
	The thermal conductivity can also be expressed as a negative since heat always flows from warm to cold.
	Fourier's Law can then be written as in Equation \ref{heat_eq}
	
	
	\begin{equation}
	\label{fourier_eq}
		q = k \frac{\Delta T}{L}
	\end{equation}
	
	\begin{equation}
	\label{heat_eq}
		q = -k \frac{\partial{u}}{\partial{x}}
	\end{equation}

Derivation of the heat conduction equation over time leads to the heat diffusivity equation (Equation \ref{heatdifeq})

\begin{equation}\label{heatdifeq}
\frac{\partial u}{\partial t} = -\alpha \frac{\partial^2 u}{\partial x^2}
\end{equation}
where 
\begin{equation}\label{alphaeq}
\alpha = \frac{\kappa}{c_p\rho}
\end{equation}
From Equation \ref{alphaeq} the importance of knowing the specific heat cpacity and dentsity of the timber at different temperatures is clear.
The values in Table \ref{cptab} will be used in conjunction with the $\kappa$-values determined through simulation to calculate new thermal diffusivity values.

\section{Finite Element Method}\label{femsec}
The finite element method (or finite element analysis) is  used when the behaviour of a large element cannot be accurately depicted by a simple mathematical equation. 
Modelling the behaviour of large and complex systems is a near impossible task without breaking the problem into smaller understandable parts.
The finite element method enables engineers and scientists to break a large problem or element into comprehend-able and definable parts \citep{Zienkiewicz:2000}. 
This allows approximations and assumptions to be made at a small scale without affecting the large scale significantly, such as assuming linear behaviour between nodes. 
To understand simply why it works you could simply look at your computer screen. 
The screen consists of pixels that are simplified to one colour but the collection of pixels that is seen from the user perspective can show complex forms and pattens.
Each pixel was simplified and approximated but the end result is sufficiently accurate due to the sheer number of pixels.
Assumptions made on a smaller scale have a lesser effect on the final answer than the same assumptions made on a large scale would have had.
If the whole screen was one pixel and only the majority colour was assumed true the whole screen would be white and that is clearly incorrect. 
From that follows that the accuracy of a finite element model is very dependant on the number of elements used and only improves with more elements.
Sadly the computational time needed to analyse a model with a thousand elements is significantly more than needed for a model with less elements.
The fine balance between sufficient accuracy and short enough computational time should always be kept in mind.


	\subsection{Origin}
	The finite element method (FEM) used today is the sum of decades of research. 
	In an article by \citet{Gupta:1996}, the authors discuss the five main contributors to the finite element method. 
	They claim the idea behind the finite element method was initially explored in the \citeyear{Courant:1943} article by \citeauthor{Courant:1943}. 
	Courant acknowledges the complex nature of mathematical problems in his first paragraph by stating: "Mathematics is an indivisible organism uniting theoretical contemplation and active application."
	He goes on to discuss the variational method created by Lord Rayleigh and Walther Ritz and the shortcomings of their methods.
	He concludes that using few elements does not provide accurate local estimations but instead the interpolation will provide approximations of the actual quantities that are sufficient for engineering problems.
	As his paper did not go into depth into the calculations Courant cannot be accredited with the full development but rather with the starting point.
	In 1954 J. Argyris published an article  where the details of his calculations are clear and matrix formulation can be seen.
	The finite element method as known to day were directly defined and clarified in a text book by Zienkiewicz in 1957.
	This textbook is now in its fifth edition of print and has been continuously updated and was used for this project.
	
	\subsection{Boundaries}
	An important part of how finite element models are how the known information is embedded into the model.
	The main way this is done is by defining boundary conditions at known points. The two types of boundary conditions used in this project are Dirichlet boundaries and Newmann boundaries.
	
	Named after Johann P.G.L Dirichlet \citep{Cheng:2005}, Dirichlet boundaries force the solution function to be equal to the prescribed value at the boundary. 
	Newmann boundaries prescribe that the derivative of the solution function be equal to the predetermined value. 
	In the context of this problem, the Dirichlet boundaries prescribe the temperatures at the boundaries and the Newmann boundaries prescribe the heat flux at the boundaries.
	
%
%	\subsection{Weak form and Strong form}
%	The weak form is obtained from the balance equation and the natural boundaries\cite{Fish:2007}
%	

	
\section{Bayes' theorem of inverse problems}

%	Statistical and Computational Inverse problems by Kaipio and Somersalo Chapter 3
% 	The Bayesian approach to Inverse Problems Dashti and Stuart
	The method of statistical inversion is dependant on a fundamental understanding of the Bayes' theorem of inverse problems. 
	The student obtained this understanding through studying Chapter 3 of statistical and Computational Inverse problems by \citet{Kaipo:2005}, further referred to merely as Kaipio. 
	There are four principles of Statistical inversion that is essential to the thorough understanding of these models. 
	Firstly, it is the principle that any variable in the model needs to be modelled as a random variable. 
	This randomness is based on the extent of information that is available. 
	To ensure that the extent of knowledge is accurately portrayed in the model, the extent of knowledge will be coded into the probability distributions assigned to the different variables. 
	Finally, it needs to be understood that the solution of a statistical inversion is a posterior probability distribution.
	A generalized equation of Bayes' theorem can be seen in \ref{bayes_eq} taken from Kaipio. 
	
	\begin{equation}
	\label{bayes_eq}
	\pi_{\text{post}}(x) = \pi(x|y_{\text{observed}}) = \frac{\pi_{\text{pr}}(x) \pi(y_{\text{observed}}|x)}{\pi (y_{\text{observed}})}	
	\end{equation}
	
Further application of Bayes' theorem of inverse problems to the project at hand is discussed in Section \ref{secInvmet}
\section{Markov Chain Monte Carlo} \label{MCMCdet}
Markov Chain Monte Carlo (MCMC) is a method of integration that will be used to determine the mean of the $\kappa$-values at specific temperatures. 
	Markov Chain Monte Carlo is a method that was created by combining the concept of Monte Carlo sampling  and a Markov Chain. 
	To fully understand MCMC, its underlying methods must be investigated further.
	For a better understanding of this concept, \textit{Introducing Markov chain Monte Carlo} by \citeauthor{Gilks:1996}, Kaipio, and various websites \citep{MLM:2019}, \citep{dummies:2015} were consulted.(RIJK: is sin lomp?)
	\subsection{Markov Chains}	\label{markovexpl}
	The core principle of a Markov chain is that the next value ($x_{n+1}$) in a sequence is dependent on the current value($x_n$). 
	A step size that indicates the range within the next point falls in.
	Values are then randomly generated but restricted to be within this range.
	This concept can be visualised as follows: the accepted point ($x_1$) is in the centre of a cube. The next possible random point is randomly generated but still within the cube(our search range). 	
	After this next number is selected, the cube moves such that the new point($x_2$) is now the centre, and so it continues.
	See Figure \ref{cubeexplfig} for clarification.
	The above example simplifies the concept, but this understanding can now be expanded.
	If every coordinate direction in the aforementioned simple example is seen as a single entry in the $x$ vector, then the example has three independent values.
	More or fewer values can be used depending on the problem.
	Another level of complication can be added if it is taken into account that every point in the cube is no longer equally likely.
	A distribution within the cube can be chosen, for example simply a normal distribution
	The shape of the cube then warps into a stranger shape with points closer to the center being more likely choices and the edges being less likely.
	\begin{figure}	
	\centering
	\includegraphics[width=0.55\linewidth]{figures/MC_cubes.png}
	\caption{Three-dimensional example of Markov Chain application (Created on \url{https://www.geogebra.org/3d})}
	\label{cubeexplfig}
	\end{figure}
	
	%%BURN_IN TODO
	The purpose of a Markov Chain is for the chain to converge to a distribution and be independent on the very initial estimation. 
	In principle, it should then reach a near stationary distribution.
	Since Markov Chains are not used if we know the answer, a way to determine when values are no longer affected by the initial estimate is needed \cite{Gilks:1996}. 
	The simple proposed solution is the concept of burn-in. 
	The concept of conventional burn-in for usage in Markov Chains are disputed as the Markov Chain itself is created in such a way that values are only directly dependent on the value immediately before them \cite{Meyn:1993}.
	Burn-in in the Markov Chain sense can simply be referred to as the removal of the initial samples of low probability to increase the accuracy of the average taken after all the iterations \cite{John:2016}.
		
	\subsection{Monte Carlo Integration}\label{MCint_sec}
	Monte Carlo integration is used to evaluate a probability distribution that cannot be solved simply. 
	The evaluation is done by drawing a collection of random values from the distribution.
	These values are then used as the sample, and a sample mean is taken.
	The arithmetic sample mean can be used to approximate the population mean per the law of large numbers \citep{Gilks:1996}.
	

\subsection{Metropolis-Hastings Algorithm}

The Metropolis-Hastings algorithm is one of the available simulation methods based on the MCMC principles. 
For this project, the Metropolis-Hastings algorithm was chosen above the Gibbs-sampler as the Gibbs sampler can be relatively slow \cite{Murphy:2012}.

All of the random samples generated by the Monte Carlo integration can not be immediately accepted.
	Here, the acceptance criterion comes into play.
	There are multiple options for how a posterior is deemed acceptable; these are elaborated on in the book  \textit{Monte Carlo Statistical Methods} by \citeauthor{Robert:2004}. 
	The most general acceptance criterion is set out in Equation \ref{acceptcriteq} and comes from Kaipio.
	
		\begin{equation} \label{acceptcriteq}
		\begin{aligned}
		&\text{if  }\quad \frac{\pi (x_2)}{\pi(x_1)} > 1 \quad \text{Accept automatically}\\
		&\text{or  }\quad \frac{\pi (x_2)}{\pi(x_1)} > \text{rand}  \quad \text{Accept}\\
		&\text{else reject and reselect  } x_2
		\end{aligned}
		\end{equation}

\section{Maximum a Posteriori}
To ensure a thorough investigation of the thermal diffusivity of timber the posteriori function will also be optimised to obtain the value with the highest probability, maximum a posteriori, in addition to the mean.
As the implementation will be done using built-in optimisation functions in Matlab the technical aspects will briefly explain how that function works.
The function used is \texttt{fminsearch.m}.
In the name of the function, it can be seen that the function will be minimised.
This seems counter-intuitive since the intention is to find the maximum of the function. 
Luckily the minimum is simply a negative maximum. 

According to the \texttt{fminsearch} Matlab documentation (\url{https://ch.mathworks.com/help/matlab/ref/fminsearch.html}) , the algorithm is based on the Nelder-Mead algorithm \citep{Lagarias:1998}.
The Nelder-Mead algorithm is an unconstrained heuristic direct search method.
The heuristic aspect of the algorithm means that it searches the function space in a guided random manner.

Nelder-Mead uses a simplex, a triangle in the dimension needed, to find the minimum.
In two-dimension, the simplex is simply a triangle, where each point has a $x$ and $y$ coordinate and is notated as. 
A starting value needs to be provided to the algorithm, this will then be one of the points in the first simplex.
All the points on the simplex are ordered from best to worst, where the smallest value ($f(x)$) is best if minimising.
In the two-dimensional example, the centroid of the two best points ($x_l,x_s$) are calculated, from there using the value of the function at the simplexes and the centroid transformations can be performed to find even smaller values.
The transformations are applied to change the worst point ($x_h$) until it is no longer the worst.

The transformations are performed in a specific order.
The first transformation is reflection.
Point $x_h$ is mirrored around the line that passes through the centroid and $x_l$ and $x_s$ to create $x_r$.
The idea behind this is that the values are lower in that direction so the algorithm attempts to take the biggest stride possible to a lower value.
If $x_r$ is better than $x_r$  the triangle is expanded and $x_e$ is even further in the assumed minimising direction.
If $x_r$ is not better than $x_h$, the reflected point is rejected and the original $x_3$ is contracted closer to $x_1$ and $x_2$.
This contracted point ($x_c$) should ideally have a smaller value than $x_h$, if not a shrink contraction needs to be performed.
In a shrink contraction, $x_h$ and $x_s$ are redefined and pulled closer to $x_l$ with hopes of finding the minimum near the best point.

The result is a simplex that moves over a surface in search of the lowest point and as it reaches that point it shrinks until all the points are practically at the same place. When this happens or the points are close enough to each other as defined when initializing the code the algorithm stops. Below in Figure \ref{neldermeadfig}, the different transformations can be seen.

\begin{figure}\label{neldermeadfig}
\centering
\includegraphics[width=\linewidth]{figures/MAP_expl.png}
\caption{Visual representation of Nedler-Mead transformations a) Reflection, b) Expansion, c) Contraction, d) Shrink contraction (Source:\url{https://codesachin.wordpress.com/2016/01/16/nelder-mead-optimization/})}
\end{figure}

%gramarly done 17/10

